# AI Security Portfolio

**Author:** <Ajay Kumar Samudrala> — MS Computer Science

Reproducible demos that break and defend ML models. Focus: Adversarial ML, Data Poisoning, Privacy-Preserving ML, and applying NIST AI RMF to real models.

## Quick links
- Projects (folder): `/project-mnist`, `/project-poisoning`, `/capstone`
- Blog: short writeups & lessons learned
- Resume: `resume_ai_security.pdf`
- Contact: LinkedIn — linkedin.com/in/samudrala-ajay-kumar-668065230

## Quick start (Day 1 → Day 3)
Prereqs: Python 3.10+, git, (docker optional)
1. `git clone https://github.com/SamudralaAjayKumarrr/ai-security-portfolio.git`
2. `cd ai-security-portfolio/project-mnist`
3. `python3 -m venv venv && source venv/bin/activate`
4. `pip install -r requirements.txt`
5. `jupyter notebook notebooks/01_train_mnist.ipynb`

## Structure (plan)
- `/project-mnist` — train model, FGSM & PGD attacks, adversarial training
- `/project-poisoning` — small poisoning/backdoor demo + detection heuristics
- `/capstone` — end-to-end deploy + audit + one-pager (NIST RMF)
- `/notes` — NIST RMF summary, papers, reading notes

## Contact
LinkedIn: linkedin.com/in/samudrala-ajay-kumar-668065230
